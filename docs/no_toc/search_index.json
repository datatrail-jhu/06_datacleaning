[["index.html", "06: Data Tidying About this Course", " 06: Data Tidying March, 2023 About this Course This course is part of a series of courses for DataTrail. DataTrail is a no-cost, paid 14-week educational initiative for young-adult, high school and GED-graduates. DataTrail aims to equip members of underserved communities with the necessary skills and support required to work in the booming field of data science. DataTrail is a fresh take on workforce development that focuses on training both Black, Indigenous, and other people of color (BIPOC) interested in the data science industry and their potential employers. Offered by the Johns Hopkins Bloomberg School of Public Health, in partnership with local non-profits and Leanpub, DataTrail combines a mutually-intensive learning experience (MILE) with a whole-person ecosystem of support to allow aspiring data scientists and their employers to succeed. DataTrail uses mutually-intensive learning DataTrail joins aspiring data science scholars and expert-level data scientist mentors in a mutually-intensive learning experience (MILE). In the DataTrail MILE: Scholars engage in cutting-edge technical and soft skills training needed to enter the data science field. Mentors engage in anti-racism and mentorship training needed to be impactful mentors and informed colleagues on diverse data science teams. The social connections created along the way will fuel job opportunities for scholars and foster a more diverse, equitable, and inclusive climate at the mentors’ institutions. { course-completeness: 100 course-attempts: 2 default-quiz-attempts: 2 default-random-choice-order: true default-quiz-show-answers: none } "],["what-is-data.html", "Chapter 1 What is data?", " Chapter 1 What is data? The basics of data were covered in the Introductory course, where we defined data as “any information that you can store on a computer.” Examples we discussed previously of data were text messages, Facebook posts, websites you visit, things you buy with a credit card, pictures of your car on speed cameras, and information you fill out in profiles for your work, school, or community organizations. We said that if you can take a picture of it, write about it, make a video of it, or record it on audio - then it is probably data. All of this information can be collected and saved on a computer. This definition of data is still true, and it is the definition we’re going to use. In this lesson, we will discuss what data are in a bit more detail. 1.0.1 Types of data Generally, there are just two types of data. If data are numerical, consisting of counts or measurements, they’re referred to as quantitative data. If they are not numerical, they’re qualitative. Qualitative data would include words or text, but they could also include photographs, videos, or audio recordings. Every photo you have ever taken with your phone is data, and more specifically, it’s qualitative data. Examples of: Qualitative data - eye color, gender, TRUE/FALSE responses, hometown, photos, text files, audio files, videos, etc. Quantitative data - height, weight, heart rate, daily step count, body temperature, test scores, etc. While people often distinguish between qualitative data and quantitative data, the truth is that qualitative data can become quantitative data. For example, if you collect eye color information from individuals in each country around the world, you could then summarize how many people in each country have brown eyes. Suddenly, you have quantitative data - a number of individuals in each country with brown eyes. We will be working with lots of different data sets throughout this curriculum. Knowing the distinction between qualitative and quantitative data is not crucially important. But as a data scientist, it will be crucially important for you to know how to work with a lot of different data sets. Throughout these courses, you’ll do projects to familiarize yourself with how to work with data and get practice working with various different data sets to ensure that you can work with data, no matter what type it is. 1.0.2 Why is data important? You generate both qualitative and quantitative data all the time. The taps on a touchscreen at the bank are data. The GPS from the maps app in your phone generate data about you. And, every credit card purchase you have ever made has generated data. Taken together, data can tell you a lot about a person, a company, or an entire population. Data are being used to make business decisions, to decide who to advertise a product to, and to decide how much someone should pay for insurance. As a data scientist, you will be using data to answer interesting questions. The role of the data scientist is to be able to collect and clean the data, study the data, create models to help understand and answer the question, and to share your findings with other people. We will work through each of these steps throughout this curriculum; however, understanding the importance of data and what qualifies as data is an important first step. 1.0.3 Slides and Video Automated Video Slides "],["data-in-r.html", "Chapter 2 Data in R", " Chapter 2 Data in R You’ve already been working with data throughout this set of courses. You’ve generated your own data in Google Sheets (i.e. collected data from Leanpub). You’ve worked with datasets that are included in RStudio automatically (i.e. iris and mtcars). And, you’ve generated R Markdown documents and pushed code to GitHub, creating even more data! But, we haven’t formalized exactly what all the different types of data are in R, how to access them, and what the various file types in R are. We’ll discuss all of that in this lesson. Then, throughout the following lessons in this course, you’ll learn how to work with data in R. Specifically, you’ll learn how to get data into a usable format for data analysis. This process is called data wrangling, and we’ll use datasets available to you from R and RStudio to learn the necessary tools for you to excel at data wrangling. Finally, to give you an idea of where we’re going, once you have a set of skills down to functionally work with data, in the next course in this series you’ll learn about lots of different places where you can get data and how to get them into R, so you can continue to practice the data wrangling skills you’ll learn here! 2.0.1 File Types in R In previous lessons we talked about R Markdown documents (which have the file extension .rmd or .Rmd) and we’ve saved R script files (which have the file extension .r or .R), but we haven’t formally discussed what all the various types of files are used for in detail or about what other possible file types exist in R. So, we’ll do that now! Before discussing further details about each type of file, we’ll point out that the file extension (what comes after the period in the file name) specifies what type of file you’re working with. While the extension matters (.R is different than .Rmd), capitalization does not matter for file extensions (.r is the same as .R). Just something to keep in mind as you start working with these types of files. That said, while the computer will know what type of file it is regardless of capitalization, when you’re writing code, capitalization does matter. If you try to read in a file named “script.r” but accidentally specify “script.R”, your code won’t be able to find the file. We mention this now so as you start saving files you save files using consistent capitalization). The capitalization in each of the sections below is the capitalization we recommend. We’ll summarize this here before walking through each one. For following file extensions, we recommend the following capitalization: Scripting files: .R R Markdown: .Rmd Saved R Objects: .rda Serialized R Objects: .rds R Project files .Rproj 2.0.1.1 R Script file (.R) The most basic file type in R uses the .R file extension. R Script files contain code written in the R programming language. As you want to write R code and save this code, you’ll save it as a .R file. To get started writing an R Script file, you’ll go to rstudio.cloud and click on File &gt; New File &gt; RScript along the menu at the top. This will open ups a blank R Script file. R Script In this R Script is where you would write R code. As you write code, you’ll want to save this file. Here, in this example, you can see the R code we’ve written and that this file has been saved as “script_test.R.” .R file 2.0.1.2 R Markdown (.Rmd) We’ve also introduced R Markdown files previously. As a refresher, R Markdown documents use Markdown syntax and include code chunks with R code that will be run when the document is “Knit” into its final format (which can be PDF, HTML, or Word documents among others. To get started writing an R Markdown document, you’ll go to rstudio.cloud and click on File &gt; New File &gt; R Markdown… After filling out the information in the pop-up window and clicking OK, an R Markdown document will open up. R Markdown In this document you’ll use Markdown syntax outside of code chunks and R code in the code chunks to generate your reports. .Rmd file R Markdown documents are great at producing reproducible reports that walk people through your data science process from start to finish. 2.0.1.3 Saved R objects (.rda, .RData) As you write R code (say, in an .R file), you will inevitably create objects. For example, in the code you see here, mpg is an object created by this code that contains the data from the mpg column of the mtcars data frame and cyl an object from the cyl column of the mtcars data frame. mpg &lt;- mtcars$mpg cyl &lt;- mtcars$cyl mpg and cyl are objects If I were to run these two lines of code, they would show up in the Console, and the objects mpg and cyl would now be visible in the Environment tab. mpg and cyl visible in Environment tab Often you’ll create objects that you won’t need to save. However, from time to time you’ll make changes to a data frame or generate an object that you’ll want to use later. To save an object and use it later, you can save it as an R data file. These files contain objects that can be saved directly from and read directly into R using the commands save() and load(). And, in particular, R data files can save multiple objects. To save the two objects mpg and cyl into an R Data file, you would do the following: save(mpg, cyl, file=&quot;mtcars_objects.rda&quot;) save The syntax for save requires you to specify the objects you want to save, separated by commas. Then, the filename you want this R data file to have is specified using the file= argument. When this line of code is run, the specified file (here, mtcars_objects.rda) appears in the Files tab within RStudio, demonstrating that this file has been created. If in the future, if you were to want to load the objects mpg and cyl back into your R session, you would do this using load(). To do that here, we’ll first have to remove these objects from RStudio. To accomplish this, click on the broom in the Environment tab of RStudio Cloud. broom icon removes all objects from R session After clicking “Yes” to confirm that you want to remove all objects, you’ll see that mpg and cyl are no longer in your RStudio environment. They have been removed from RStudio. Empty Environment To load them back in from mtcars_objects.rda, you would do the following: load(&quot;mtcars_objects.rda&quot;) Note that you need to put quotes around the filename to specify that this is a file and not an object in R. A second note is a reminder about file paths. Here, the file we want is in the directory we’re working in. If that were not the case, you would have to specify the path to the file within the quotation marks (i.e. “path/to/file/mtcars_objects.rda”) When you run this line of code, you’ll see that cyl and mpg are once again available objects in your RStudio Environment tab! load loads objects in .rda file back in To recap, when you want to save multiple objects to be read in at a later time, you’ll save them as an R data file (.rda). R Data files are generated using the save() function and read into R using the load() command. 2.0.1.4 Serialized R objects (.rds) Serialized R object files are similar to R data files with the exception that they can only save a single R object at a time. When you only want to save a single object (say, mpg), these files are the best option. To save a serialized R object file, you’ll use the function saveRDS(). To read a serialized object file into R, you’ll use the function readRDS(). For example, to save the mpg object you would use the following: saveRDS(mpg, file=&quot;mpg.rds&quot;) saveRDS This will generate the file mpg.rds. To read this file back in, we’ll first remove all objects from R Studio using the broom icon in the Environment tab and will then use the readRDS() function. broom icon to clean up Environment Unlike with .rda objects, when you read a .rds object back in, to make the object available, you have to assign the object a variable name. Otherwise, the contents of the object will just print to your Console. This is helpful because it doesn’t matter what the name of the object was when you saved it. As long as you know the filename, you can pick a new object name. Here, we assign the serialized R object file to mpg, but we could have chosen a different object name. mpg &lt;- readRDS(&quot;mpg.rds&quot;) readRDS 2.0.1.5 R Project files (.Rproj) R Projects are incredibly helpful files. In their simplest form, by working within an R Project and saving an .Rproj file, you are always able to pick back up on a project from where you left off. Fortunately, RStudio Cloud uses R projects automatically. You may have noticed that there is a “project.Rproj” file in your Files tab. This is an R Project file. RStudio Cloud will start a “project.Rproj” file within each project you start. This file will then track your project as you write code and create objects. R Project files Then, when you return to this project, your .Rroj file will: start a new R session load the .RData file in your project’s main directory file if there is one (this is the same directory where your .Rproj file is) load the .Rhistory file into the RStudio History tab set the current working directory to the project directory. make previously edited documents available in the Files tab restore RStudio settings to what they were the last time the project was closed. This allows you to pick back up from where you left off! Then, when you leave to work on a different project, your .RData will be saved and your .Rproj file updated so that you can pick up from where you left off the next time you sign in! 2.0.2 Datasets in R Now that we’ve covered the main types of R files you’ll be working with and generating in RStudio, we’ll move on to formally discussing the datasets available within R and RStudio/RStudio Cloud. You’ve previously seen and worked with datasets that are available to you in R. Specifically, you’ve seen examples that used the iris and mtcars datasets in previous lessons. However, we haven’t covered how to find all available datasets that are included in R automatically, so we’ll do that now. To access a list of all the available datasets within R, you’ll type data() into your Console. This will open up a file called “R data sets”. You can scroll through this list to get a sense of what data sets are available. The name of the data set is in the first column while a brief description of the dataset is in the second column. R data sets If one of these datasets looks interesting to you, say the “Orange” dataset, you can get more detailed information about the dataset by using help function in R (?). ?Orange This opens up an explanation in the Help tab of RStudio Cloud. Here, you can see a description of the dataset and can scroll to see what variables are included in this dataset. Finally, at the bottom of the Help window, you’ll see examples of how to work with the dataset! Orange dataset Help page 2.0.3 Datasets in Packages In addition to the datasets available by default to you in R, many packages (but not all!) also contain datasets that you can use. To see a list of what datasets are available from a specific package, you can again use the data() function, but you’ll want to specify the name of the package using the package= argument For example, to see a list of the datasets available from the ggplot2 package, you would want to use the following: data(package=&quot;ggplot2&quot;) datasets available from ggplot2 package As before, this function will open up a list of the available datasets within that package. In order to use any of these datasets, however, you’ll have to load the package using the library() function. After loading the package, you can then access the Help pages for a dataset as previously using ?. library(ggplot2) ?msleep msleep from ggplot2 package dataset Help page 2.0.4 Other types of data in R In addition to datasets included automatically in R/RStudio and those that are in R packages, there are lots of other types of data that can be read into R. This could be data from a Google Sheet or an Excel spreadsheet or data from a website. In the lessons in the next course we’ll discuss a number of other types of files and master how to get them into R, so just know that’s coming up! For the remaining lessons in this course, however, we’ll stick to using datasets included in R and those included in specific R packages. 2.0.5 Summary In this lesson we’ve covered the main file formats that are used within R. Make sure you’re comfortable with what the various file extensions mean and in what scenarios you would use the different file extensions. Additionally, we discussed how to access datasets that are available by default in RStudio as well as those that are available through specific R packages. You already have so much data at you fingertips, and we’re just getting started! 2.0.6 Additional Resources R Studio Projects R Data files 2.0.7 Slides and Video Automated Video Slides "],["tidy-data.html", "Chapter 3 Tidy Data", " Chapter 3 Tidy Data The idea of tidy data was formalized in 2014 in a paper written by a leader in the data science field, Hadley Wickham. The principles of tidy data, which are discussed below, provide a standard way of formatting a data set. A tidy dataset follows a number of rules relating to how rows, columns, and spreadsheets are matched up with observations, variables, and types. 3.0.1 Why Tidy Data? Tidy data-sets, by design, are easier to manipulate, model, and visualize. By starting with data that are already in a tidy format or by spending the time at the beginning of a project to get data into a tidy format, the remaining steps of your data science project will be easier. 3.0.2 Data Terminology We’ve previously discussed what the rows and columns in a spreadsheet are. Here, we’ll discuss what is meant by observations, variables, and types, all of which are used to explain the principles of tidy data. 3.0.2.1 Variables Variables in a spreadsheet are the different categories of data that will be collected. They are the different pieces of information that can be collected or measured on each person. Here, we see there are 7 different variables: ID, LastName, FirstName, Sex, City, State, and Occupation. The names for variables are put in the first row of the spreadsheet. Variables 3.0.2.2 Observations The measurements taken from a person for each variable are called observations. Observations for each individual are stored in a single row, with each observation being put in the appropriate column for each variable. Observations 3.0.2.3 Types Often, data are collected for the same individuals from multiple sources. For example, when you go to the doctor’s office, you fill out a survey about yourself. That would count as one type of data. The measurements a doctor collects at your visit, however, would be a different type of data. Types 3.0.3 Principles of Tidy Data Each variable you measure should be in one column. Principle #1 of Tidy Data Each different observation of that variable should be in a different row. Principle #2 of Tidy Data There should be one spreadsheet for each “type” of data. Principle #3 of Tidy Data If you have multiple spreadsheets, they should include a column in each spreadsheet (with the same column label!) that allows them to be joined or merged. Principle #4 of Tidy Data 3.0.4 Rules for Tidy Spreadsheets In addition to these four principles, there are a number of rules to follow when entering data into a spreadsheet, or when re-organizing untidy data that you have already been given for a project into a tidy format. They are rules that will help make data analysis and visualization easier down the road. They were formalized in a paper called “Data organization in spreadsheets”, written by two prominent data scientists, Karl Broman and Kara Woo. In this paper, in addition to ensuring that the data are tidy, they suggest following these guidelines when entering data into spreadsheets: Be consistent Choose good names for things Write dates as YYYY-MM-DD No empty cells Put just one thing in a cell Don’t use font color or highlighting as data Save the data as plain text files We’ll go through each of these to make sure we’re all clear on what a great tidy spreadsheet looks like. 3.0.4.1 Be consistent Being consistent in data entry and throughout an analysis is key. It minimizes confusion and makes analysis simpler. For example, here we see sex is coded as “female” or “male.” Those are the only two ways in which sex was entered into the data. This is an example of consistent data entry. You want to avoid sometimes coding a female’s sex as “female” and then entering it as “F” in other cases. Simply, you want to pick a way to code sex and stick to it. With regard to entering a person’s sex, we were talking about how to code observations for a specific variable; however, consistency also matters when you’re choosing how to name a variable. If you use the variable name “ID” in one spreadsheet, use the same variable name (“ID”) in the next spreadsheet. Do not change it to “id” (capitalization matters!) or “identifier” or anything else in the next spreadsheet. Be consistent! Consistency matters across every step of the analysis. Name your files in a consistent format. Always code dates in a consistent format (discussed further below). Avoid extra spaces in cells. If you’re careful about and consistent in data entry, it will be incredibly helpful when you get to analysis. Be Consistent! 3.0.4.2 Choose good names for things Choosing good variable names is important. Generally, avoid spaces in variable names and file names. You’ll see why this is important as we learn more about programming, but for now, know that “Doctor Visit 1” is not a good file name. “doctor_visit_v1” is much better. Stick to using underscores instead of spaces or any other symbol when possible. The same thing goes for variable names. “FirstName” is a good variable name while “First Name” with a space in the middle of it is not. Additionally, make sure that file and variable names are as short as possible while still being meaningful. “F1” is short, but it doesn’t really tell you anything about what is in that file. “doctor_visit_v1” is a more meaningful file name. We know now that this spreadsheet contains information about a doctor’s visit. ‘v1’ specifies version 1 allowing for updates to this file later which would create a new file “doctor_visit_v2.” Choose good names 3.0.4.3 Write dates as YYYY-MM-DD When entering dates, there is a global ‘ISO 8601’ standard. Dates should be encoded YYYY-MM-DD. For example if you want to specify that a measurement was taken on February 27th, 2018, you would type 2018-02-27. YYYY refers to the year, 2018. MM refers to the month of February, 02. And DD refers to the day of the month, 27. This standard is used for dates for two main reason. First, it avoids confusion when sharing data across different countries, where date conventions can differ. By all using ISO 8601 standard conventions, there is less room for error in interpretation of dates. Secondly, spreadsheet software often mishandles dates and assumes that non-date information are actually dates and vice versa. By encoding dates as YYYY-MM-DD, this confusion is minimized. YYYY-MM-DD 3.0.4.4 No empty cells Simply, fill in every cell. If the data is unknown for that cell, put ‘NA.’ Without information in each cell, the analyst is often left guessing. In the spreadsheets below, on the left, is the analyst to assume that the empty cells should use the date from the cell above? Or are we to assume that the date for that measurement is unknown? Fill in the date if it is known or type ‘NA’ if it is not. That will clear up the need for any guessing on behalf of the analyst. On the spreadsheet to the right, the first two rows have a lot of empty cells. This is problematic for the analysis. This spreadsheet does not follow the rules for tidy data. There is not a single variable per column with a single entry per row. These data would have to be reformatted before they could be used in analysis. No empty cells 3.0.4.5 Put just one thing in a cell Sometimes people are tempted to include a number and a unit in a single cell. For weight, someone may want to put ‘165 lbs’ in that cell. Avoid this temptation! Keep numbers and units separate. In this case, put one piece of information in the cell (the person’s weight) and either put the unit in a separate column, or better yet, make the variable name weight_lbs. That clears everything up for the analyst and avoids a number and a unit from both being put in a single cell. As analysts, we prefer weight information to be in number form if we want to make calculations or figures. This is facilitated by the first column called “Weight_lbs” because it will be read into R as a numeric object. The second column called “Weight”, however, will be read into R as a character object because of the “lbs”, which makes our desired tasks more difficult. One thing per cell 3.0.4.6 Don’t use font color or highlighting as data Avoid the temptation to highlight particular cells with a color to specify something about the data. Instead, add another column to convey that information. In the example below, 1.1 looks like an incorrect value for an individual’s glucose measure. Instead of highlighting the value in red, create a new variable. Here, on the right, this column has been named ‘outlier.’ Including ‘TRUE’ for this individual suggests that this individual may be an outlier to the data analyst. Doing it in this way ensures that this information will not be lost. Using font color or highlighting however can easily be lost in data processing, as you will see in future lessons. No highlighting or font color 3.0.4.7 Save the data as plain text files The following lessons will go into detail about which file formats are ideal for saving data, such as text files (.txt) and comma-delimited files (.csv). These file formats can easily be opened and will never require special software, ensuring that they will be usable no matter what computer or analyst is looking at the data. 3.0.4.8 Tidy Data Summary The data entry guidelines discussed here and a few additional rules have been summarized below and are available online for reference. Naming Guidelines Most importantly, however, remember that tidy data are rectangular data. The data should be a rectangle with each variable in a separate column and each entry in a different row. All cells should contain some text, so that the spreadsheet looks like a rectangle with something in every cell. Tidy Data = rectangular data 3.0.5 Additional resources Hadley Wickham’s paper on Tidy Data Data Organization in Spreadsheets How to Share Data for Collaboration Data Carpentry’s Data Organization in Spreadsheets Karl Broman’s Data Organization blog post 3.0.6 Slides and Video Automated Video Slides "],["untidy-data.html", "Chapter 4 Untidy Data", " Chapter 4 Untidy Data We’ve just spent a while discussing the principles of tidy data as well as a number of guidelines on how to correctly enter data into spreadsheets. At this point, you may think “I got it. Make my data rectangular! I’ll do it.” But, the reality is that most data are untidy. If you are not the one entering the data but are instead handed the data from someone else to do a project, more often than not, those data will be untidy. Untidy data are often referred to simply as messy data. The following common problems seen in messy data sets again come from Hadley Wickham’s paper on tidy data. After briefly reviewing what each common problem is, we will then take a look at a few messy data sets. We’ll finally touch on the concepts of tidying untidy data, but we won’t actually do any practice yet. That’s coming soon! 4.0.1 Common problems with messy data sets Column headers are values but should be variable names. A single column has multiple variables. Variables have been entered in both rows and columns. Multiple “types” of data are in the same spreadsheet. A single observation is stored across multiple spreadsheets. 4.0.2 Examples of untidy data To see some of these messy datasets, let’s explore three different sources of messy data. 4.0.2.1 Examples from Data Organization in Spreadsheets In each of these examples, we see the principles of tidy data being broken. Each variable is not a unique column. There are empty cells all over the place. The data are not rectangular. Data formatted in these messy ways are likely to cause problems during analysis. Examples from Data Organization in Spreadsheets For a specific example, Miles McBain, a data scientist from Brisbane, Australia set out to analyze Australian survey data on Same Sex marriage. Before he could do the analysis, however, he had a lot of tidying to do. He annotated all the ways in which the data were untidy, including the use of commas in numerical data entry, blank cells, junk at the top of the spreadsheet, and merged cells. All of these would have stopped him from being able to analyze the data had he not taken the time to first tidy the data. Luckily, he wrote a Medium piece including all the steps he took to tidy the data. Miles McBain’s’ tidying of Australian Same Sex Marriage Postal Survey Data Inspired by Miles’ work, Sharla Gelfand decided to tackle a messy data set from Toronto’s open data. She similarly outlined all the ways in which the data were messy including, names and address across multiple cells in the spreadsheet, merged column headings, and lots of blank cells. She has also included the details of how she cleaned these data in a blog post. While the details of the code may not make sense yet, it will shortly as you get more comfortable with the programming language, R. Sharla Gelfand’s tidying of Toronto’s open data 4.0.3 Tidying untidy data There are a number of actions you can take on a dataset to tidy the data depending on the problem. These include: filtering, transforming, modifying variables, aggregating the data, and sorting the order of the observations. There are functions to accomplish each of these actions in R. While we’ll get to the details of the code in a few lessons, it’s important at this point to be able to identify untidy data and to determine what needs to be done in order to get those data into a tidy format. Specifically, we will focus in here on a single messy data set. This is dataset D from the ‘Data Organization in Spreadsheets’ example of messy data provided above. We note the blank cells and that the data are not rectangular. Messy data set To address this, these data can be split into two different spreadsheets, one for each type of data. Spreadsheet A included information about each sample. Spreadsheet B includes measurements for each sample over time. Note that both spreadsheets have an ‘id’ column so that the data can be merged if necessary during analysis. The ‘note’ column does have some missing data. Filling in these blank cells with ‘NA’ would fully tidy these data. We note that sometimes a single spreadsheet becomes two spreadsheets during the tidying process. This is OK as long as there is a consistent variable name that links the two spreadsheets! Tidy version of the messy data set 4.0.4 Slides and Video Automated Video Slides "],["reshaping-data.html", "Chapter 5 Reshaping Data", " Chapter 5 Reshaping Data 5.0.1 Data Formats Tidy data generally exist in two forms: wide data and long data. Both types of data are used and needed in data analysis, and fortunately, there are tools that can take you from wide-to-long and from long-to-wide. This makes it easy to work with any tidy data set. We’ll discuss the basics of what wide and long data are and how to go back and forth between the two in R. Getting data into the right format will be crucial later when summarizing data and visualizing it. 5.0.1.1 Wide Data Wide data has a column for each variable and a row for each observation. Data are often entered and stored in this manner. This is because wide data are often easy to understand at a glance. For example, this is a wide data set: Wide dataset This is a dataset we’ve looked at in a previous lesson. As discussed previously, it’s a rectangular and tidy dataset. Now, we can also state that it is a wide dataset. Here you can clearly see what measurements were taken for each individual and can get a sense of how many individuals are contained in the dataset. Specifically, each individual is in a different row with each variable in a different column. At a glance we can quickly see that we have information about four different people and that each person was measured in four different ways. 5.0.1.2 Long Data Long data, on the other hand, has one column indicating the type of variable contained in that row and then a separate row for the value for that variable. Each row contains a single observation for a single variable. Below is an example of a long data set: Long dataset This long dataset includes the exact same information as the previous wide dataset; it is just stored differently. It’s harder to see visually how many different measurements were taken and on how many different people, but the same information is there. While long data formats are less readable than wide data at a glance, they are a lot easier to work with during analysis. Most of the tools we’ll be working with use long data. Thus, to go from how data are often stored (wide) to working with the data during analysis (long), we’ll need to understand what tools are needed to do this and how to work with them. 5.0.2 R Packages Converting your data from wide-to-long or from long-to-wide data formats is referred to as reshaping your data. There are two primary packages in R that will help you reshape your data: tidyr and reshape2. We’ll walk through the important functions of these two packages and work through a few examples using the functions in each. However, as with most helpful packages in R, there is more functionality than what is discussed here, so feel free to explore the additional resources at the bottom to learn even more. Reshaping data For these examples, we’ll work with the airquality dataset available in R. The data in this dataset includes “Daily air quality measurements in New York, May to September 1973.” This is a wide dataset because each day is in a separate row and there are multiple columns with each including information about a different variable (ozone, solar.r, wind, temp, month, and day). We can see the first few lines of this dataset using the following code: head(airquality) Air quality dataset Again, wide data are easy to decipher at a glance. We can see that we have six different variables for each day, with each one of these variables (measurements) being stored in a separate column. 5.0.2.1 tidyr Within tidyr, there are two functions to help you reshape your data. gather(): go from wide data to long data spread(): go from long data to wide data To get started, you’ll need to be sure that the tidyr package is installed and loaded into your RStudio session. install.packages(&quot;tidyr&quot;) library(tidyr) 5.0.2.1.1 gather() As data are often stored in wide formats, you’ll likely use gather() a lot more frequently than you’ll use spread(). This will allow you to get the data into a long format that will be easy to use for analysis. In tidyr, gather() will take the airquality dataset from wide to long, putting each column name into the first column and each corresponding value into the second column. Here, the first column will be called key. The second column will still be value. ## use gather() to reshape from wide to long gathered &lt;- gather(airquality) ## take a look at first few rows of long data head(gathered) gather dataset However, it’s very easy to change the names of these columns within gather(). To do so you define what the key and value columns names should be within gather(): ## to rename the column names that gather provides, ## change key and value to what you want those column names to be gathered &lt;- gather(airquality, key=&quot;variable&quot;, value=&quot;value&quot;) ## take a look at first few rows of long data head(gathered) gather column names changed However, you’re likely not interested in your day and month variable being separated out into their own variables within the key column. In fact, knowing the day and month associated with a particular data point helps identify that particular data point. To account for this, you can exclude day and month from the variables being included in the key column by specifying all the variables that you do want included in the key column. Here, that means specifying ozone, solar.r, wind, and temp. This will keep day and month in their own columns, allowing each row to be identified by the specific day and month being discussed. ## in gather(), after key and value, you can specify which variables ## you want included in the long format ## it will leave the other variables as is gathered &lt;- gather(airquality, key=&quot;variable&quot;, value=&quot;value&quot;, ozone, solar.r, wind, temp) ## take a look at first few rows of long data head(gathered) gather specifying which variables to include in long format Now, when you look at the top of this object, you’ll see that month and day remain in the data frame and that variable combines information from the other columns in airquality (ozone, solar.r, wind, temp). This is still a long format dataset; however, it has used month and day as IDs when reshaping the data frame. 5.0.2.1.2 spread() To return your long data back to its original form, you can use spread(). Here you specify two columns: the column that contains the names of what your wide data columns should be (key=variable) and the column that contains the values that should go in these columns (value=value). The data frame resulting from spread() will have the original information back in the wide format (again, the columns will be in a different order). But, we’ll discuss how to rearrange data in the next lesson! ## use gather() to reshape from wide to long spread_data &lt;- spread(gathered, key=variable, value=value) ## take a look at the spread data head(spread_data) ## compare that back to the original head(airquality) spread data 5.0.2.2 reshape2 As with many things in R, there is more than one way to solve a problem. While tidyr provides a more general solution for reshaping data, reshape2 was specifically designed for reshaping data. The details aren’t particularly important yet, but later on as you carry out your own analyses it will be good to know about both packages. To get started using reshape2, you’ll have to first install the library and load it into your R session: ## install the package install.packages(&#39;reshape2&#39;) ## load the package into R Session library(reshape2) There are two main functions within the reshape2 package: melt(): go from wide data to long data dcast(): go from long data to wide data 5.0.2.2.1 melt() The melt() function will allow you to get the data into a long format that will be easy to use for analysis. When you melt a dataset with the default options, melt() will take every column, put the column name into a variable column, and then put the values of those variables into a value column. For the airquality data set, below we first assign the melted data frame to the object melted. Then we take a look at the top (head()) and bottom(tail()) of this melted data frame (melted). ## puts each column name into the &#39;variable&#39; column ## puts corresponding variable&#39;s value in &#39;value&#39; column melted &lt;- melt(airquality) ## let&#39;s take a look at the top of the melted data frame head(melted) ## and at the bottom of that melted data frame tail(melted) When you run this code you see that each column from the original data frame (ozone, solar.r, wind, temp,month, and day) are now repeated in the variables column, and each days’ value for that variable is now in the value column. This is now a long format dataset! melted data Now, to use month and day as identifiers as we did with tidyr above, the approach is slightly different. With gather(), you specified the column names that you wanted to gather and omitted the column names that you wanted to retain as identifiers. With melt() you will do the opposite. You will specify day and month as identifiers for the dataset and omit the remaining variables. You’ll want to use the following syntax: ## melt the data frame ## specify each row using month and day melted &lt;- melt(airquality, id.vars = c(&quot;month&quot;,&quot;day&quot;)) ## look at the first few rows of the melted data frame head(melted) melted data frame using IDs Despite the slight change in how the code was specified, the result here using melt() is the same as what was achieved above using gather(). 5.0.2.2.2 cast You’ll likely have to go from long-to-short format less frequently; however, it’s good to know there are two approaches to accomplishing this within reshape2 whenever it is necessary. acast(): taking a long frame and returning a matrix/array/vector dcast(): taking a long frame and returning a data frame To return our melted data back into its original wide form, we’ll use dcast(). The syntax here separates what should be used as an identifier for each row in the resulting wide format (month + day below) and which column includes the values that should be the column headers (variable below). These two pieces of information are separated by a tilde (~). ## to get our data back to its original form ## specify which columns should be combined to use as identifiers ## and which column should be used to specify the columns original &lt;- dcast(melted, month + day ~ variable) head(original) head(airquality) dcast to obtain original data As you can see, aside from the column order changing, the information in original is the same as what was in the data frame we started with (airquality). While reshaping data may not be the most exciting topic, having this skill will be indispensable as you start working with data. It’s best to get these skills down early! 5.0.3 Additional Resources tidyr, part of the tidyverse and developed by Hadley Wickham and Lionel Henry reshape2, developed by Hadley Wickham tidyr tutorial by Hadley Wickham reshape2 tutorial by Sean C. Anderson tidyr vs reshape2 by Alberto Giudici 5.0.4 Slides and Video Automated Video Slides "],["tidying-data.html", "Chapter 6 Tidying Data", " Chapter 6 Tidying Data So far we’ve discussed what tidy and untidy data are. We’ve (hopefully) convinced you that tidy data are the right type of data to work with. And, more than that, hopefully we’ve explained that data are not always the tidiest when they come to you at the start of a project. An incredibly important skill of a data scientist is to be able to take data from an untidy format and get it into a tidy format. We’ve started to discuss how to do this in the last lesson where we learned to reshape data. In this lesson, we’ll discuss a number of other ways in which data can be tidied and the necessary tools to tidy data. These skills are often referred to as data wrangling. They are skills that allow you to wrangle data from the format they’re currently in into the format you actually want them in. As this is an incredibly important topic, this will be a long lesson covering a number of packages and topics. Take your time working through it and be sure to understand all of the examples! data wrangling example 6.0.1 dplyr Within R, there is a package specifically designed for helping you wrangle data. This package is called dplyr and will allow you to easily accomplish many of the data wrangling tasks necessary. In this lesson, we will cover a number of functions that will help you wrangle data using dplyr: %&gt;% - pipe operator for chaining a sequence of operations glimpse() - get an overview of what’s included in dataset filter() - filter rows select() - select, rename, and reorder columns rename() - rename columns arrange() - reorder rows mutate() - create a new column group_by() - group variables summarize() - summarize information within a dataset left_join() - combining data across data frame If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;dplyr&#39;) library(dplyr) 6.0.2 tidyr We will also return to the tidyr package. The same package that we used to reshape our data will be helpful when tidying data. The main functions we’ll cover from tidyr are: unite() - combine contents of two or more columns into a single column separate() - separate contents of a column into two or more columns If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;tidyr&#39;) library(tidyr) 6.0.3 janitor The third package we’ll include here is the janitor package. This package provides tools for cleaning messy data. The main functions we’ll cover from janitor are: clean_names() - clean names of a data frame tabyl() - get a helpful summary of a variable If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;janitor&#39;) library(janitor) 6.0.4 skimr The final package we’ll discuss here is the skimr package. This package provides a quick way to summarize a data frame. We’ll discuss its most useful function here: skim() - summarize a data frame If you have not already, you’ll want to be sure this package is installed and loaded: install.packages(&#39;skimr&#39;) library(skimr) 6.0.5 The Pipe Operator Before we get into the important functions within dplyr, it will be very useful to discuss what is known as the pipe operator. The pipe operator looks like this in R: %&gt;%. Whenever you see the pipe %&gt;%, think of the word “then”, so if you saw the sentence “I went to the the store and %&gt;% I went back to my house,” you would read this as I went to the store and then I went back to my house. The pipe tells you to do one thing and then do another. Generally, the pipe operator allows you to string a number of different functions together in a particular order. If you wanted to take data frame A and carry out function B on it in R, you could depict this with an arrow pointing from A to B: A –&gt; B Here you are saying, “Take A and then feed it into function B.” In R syntax, from what you’ve seen so far, what is depicted by the arrow above would be carried out by calling the function B on the data frame object A: B(A) Alternatively, you could use the pipe operator (%&gt;%): A %&gt;% B However, often you are not performing just one action on a data frame, but rather you are looking to carry out multiple functions. We can again depict this with an arrow diagram. A –&gt; B –&gt; C –&gt; D Here you are saying that you want to take data frame A and carry out function B, then you want to take the output from that and then carry out function C. Subsequently you want to take the output of that and then carry out function D. In R syntax, we would first apply function B to data frame A, then apply function C to this output, then apply function D to this output. This results in the following syntax that is hard to read because multiple calls to functions are nested within each other: D(C(B(A))) Alternatively, you could use the pipe operator. Each time you want take the output of one function and carry out something new on that output, you will use the pipe operator: A %&gt;% B %&gt;% C %&gt;% D Below we’ll use this pipe operator a lot. Essentially, it takes output from the left hand side and feeds it into a function on the right hand side. You’ll get a better understanding of how it works as you run the code below. But, when in doubt remember that the pipe operator should be read as then. 6.0.6 Filtering Data When working with a large dataset, you’re often interested in only working with a portion of the data at any one time. For example, if you had data on people from ages 0 to 100 years old, but you wanted to ask a question that only pertained to children, you would likely want to only work with data from those individuals who were less than 18 years old. To do this, you would want to filter your dataset to only include data from these select individuals. Filtering can be done by row or by column. We’ll discuss the syntax in R for doing both. Please note that the examples in this lesson and the organization for this lesson were adapted from Suzan Baert’s wonderful dplyr tutorials. Links to the all four tutorials can be found in the “Additional Resources” section at the bottom of this lesson. For the examples below, we’ll be using a dataset from the ggplot2 package called msleep. (You’ll learn more about this package in a later course on data visualization.) This dataset includes sleep times and weights from a number of different mammals. It has 83 rows, with each row including information about a different type of animal, and 11 variables. As each row is a different animal and each column includes information about that animal, this is a wide dataset. To get an idea of what variables are included in this data frame, you can use glimpse(). This function summarizes how many rows there are (Observations) and how many columns there are (Variables). Additionally, it gives you a glimpse into the type of data contained in each column. Specifically, in this data set, we know that the first column is name and that it contains a character vector (chr) and that the first three entires are “Cheetah”, “Owl monkey”, and “Mountain beaver.” It works similarly to the summary() function covered in an earlier course. ## install packages if you haven&#39;t already install.packages(&#39;ggplot2&#39;) ## load package library(ggplot2) ## take a look at the data glimpse(msleep) Glimpse of msleep dataset 6.0.6.1 Filtering Rows If you were only interested in learning more about the sleep times of “Primates,” we could filter this dataset to include only data about those mammals that are also Primates. As we can see from glimpse(), this information is contained within the order variable. So to do this within R, we use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;) Note that we are using the equality == comparison operator that you learned about in the previous course. Also note that we have used the pipe operator to feed the msleep data frame into the filter() function. This is shorthand for: filter(msleep, order == &quot;Primates&quot;) Filtered to only include Primates Here, we have a smaller dataset of only 12 mammals (as opposed to the original 83) and we can see that the order variable column only includes “Primates.” But, what if we were only interested in Primates who sleep more than 10 hours total per night? This information is in the sleep_total column. Fortunately, filter() also works on numeric variables. To accomplish this, you would use the following syntax, separating the multiple filters you want to apply with a comma: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) Note that we have used the “greater than” comparison operator with sleep_total. Now, we have a dataset focused in on only 5 mammals, all of which are primates who sleep for more than 10 hours a night total. Numerically filtered dataset We can obtain the same result with the AND &amp; logical operator instead of separating filtering conditions with a comma: msleep %&gt;% filter(order == &quot;Primates&quot; &amp; sleep_total &gt; 10) Note that the number of columns hasn’t changed. All 11 variables are still shown in columns because the function filter() filters on rows, not columns. 6.0.6.2 Selecting Columns While filter() operates on rows, it is possible to filter your dataset to only include the columns you’re interested in. To select columns so that your dataset only includes variables you’re interested in, you will use select(). Let’s start with the code we just wrote to only include primates who sleep a lot. What if we only want to include the first column (the name of the mammal) and the sleep information (included in the columns sleep_total, sleep_rem, and sleep_cycle)? We would do this by starting with the code we just used, adding another pipe, and using the function select(). Within select, we specify which columns we want in our output. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_total, sleep_rem, sleep_cycle) Data with selected columns Now, using select() we see that we still have the five rows we filtered to before, but we only have the four columns specified using select(). Here you can hopefully see the power of the pipe operator to chain together several commands in a row. Without the pipe operator, the full command would look like this: select(filter(msleep, order == &quot;Primates&quot;, sleep_total &gt; 10), name, sleep_total, sleep_rem, sleep_cycle) 6.0.6.3 Renaming Columns select() can also be used to rename columns. To do so, you use the syntax: new_column_name = old_column_name within select. For example, to select the same columns and rename them total, rem and cycle, you would use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, total=sleep_total, rem=sleep_rem, cycle=sleep_cycle) Data with renamed columns names with select() It’s important to keep in mind that when using select() to rename columns, only the specified columns will be included and renamed in the output. If you, instead, want to change the names of a few columns but return all columns in your output, you’ll want to use rename(). For example, the following, returns a data frame with all 11 columns, where the column names for three columns specified within rename() function have been renamed. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% rename(total=sleep_total, rem=sleep_rem, cycle=sleep_cycle) Data with renamed columns names using rename() 6.0.7 Reordering In addition to filtering rows and columns, often, you’ll want the data arranged in a particular order. It may order the columns in a logical way, or it could be to sort the data so that the data are sorted by value, with those having the smallest value in the first row and the largest value in the last row. All of this can be achieved with a few simple functions. 6.0.7.1 Reordering Columns The select() function is powerful. Not only will it filter and rename columns, but it can also be used to reorder your columns. Using our example from above, if you wanted sleep_rem to be the first sleep column and sleep_total to be the last column, all you have to do is reorder them within select(). The output from select() would then be reordered to match the order specified within select(). msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) Here we see that sleep_rem name is displayed first followed by sleep_rem, sleep_cycle, and sleep_total, just as it was specified within select(). Data with reordered columns names 6.0.7.2 Reordering Rows Rows can also be reordered. To reorder a variable in ascending order (from smallest to largest), you’ll want to use arrange(). Continuing on from our example above, to now sort our rows by the amount of total sleep each mammal gets, we would use the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(sleep_total) Data arranged by total sleep in ascending order While arrange sorts variables in ascending order, it’s also possible to sort in descending (largest to smallest) order. To do this you just use desc() with the following syntax: msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(desc(sleep_total)) By putting sleep_total within desc(), arrange() will now sort your data from the primates with the longest total sleep to the shortest. Data arranged by total sleep in descending order arrange() can also be used to order non-numeric variables. For example, arrange() will sort character vectors alphabetically. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name) Data arranged alphabetically by name If you would like to reorder rows based on information in multiple columns, you can specify them separated by commas. This is useful if you have repeated labels in one column and want to sort within a category based on information in another column. In the example here, if there were repeated primates, this would sort the repeats based on their total sleep. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name, sleep_total) 6.0.8 Creating new columns You will often find when working with data that you need an additional column. For example, if you had two datasets you wanted to combine, you may want to make a new column in each dataset called dataset. In one dataset you may put datasetA in each row. In the second dataset, you could put datasetB. This way, once you combined the data, you would be able to keep track of which dataset each row came from originally. More often, however, you’ll likely want to create a new column that calculates a new variable based on information in a column you already have. For example, in our mammal sleep dataset, sleep_total is in hours. What if you wanted to have that information in minutes? You could create a new column with this very information! The function mutate() was made for all of these new-column-creating situations. This function has a lot of capabilities. We’ll cover the basics here. Returning to our msleep dataset, after filtering and re-ordering, we can create a new column with mutate(). Within mutate(), we will calculate the number of minutes each mammal sleeps by multiplying the number of hours each animal sleeps by 60 minutes. msleep %&gt;% filter(order == &quot;Primates&quot;, sleep_total &gt; 10) %&gt;% select(name, sleep_rem, sleep_cycle, sleep_total) %&gt;% arrange(name) %&gt;% mutate(sleep_total_min = sleep_total * 60) Mutate to add new column to data 6.0.9 Separating Columns Sometimes multiple pieces of information are merged within a single column even though it would be more useful during analysis to have those pieces of information in separate columns. To demonstrate, we’ll now move from the msleep dataset to talking about another dataset that includes information about conservation abbreviations in a single column. To read this file into R, we’ll use the httr package, which will be discussed in detail in a future lesson. For now, however, know that we’re using this to read in a file from the Internet using the code below. ## if not already installed, you&#39;ll have to run the following line of code ## install.packages(&#39;httr&#39;) ## install.packages(&#39;readr&#39;) ## load the libraries library(httr) library(readr) ## download file GET(&quot;https://raw.githubusercontent.com/suzanbaert/Dplyr_Tutorials/master/conservation_explanation.csv&quot;, write_disk(tf &lt;- tempfile(fileext = &quot;.csv&quot;))) conservation &lt;- read_csv(tf) ## take a look at this file head(conservation) Conservation data set In this dataset, we see that there is a single column that includes both the abbreviation for the conservation term as well as what that abbreviation means. Recall that this violates one of the tidy data principles covered in the first lesson: Put just one thing in a cell. To work with these data, you could imagine that you may want these two pieces of information (the abbreviation and the description) in two different columns. To accomplish this in R, you’ll want to use separate() from tidyr. The separate() function requires the name of the existing column that you want to separate (conservation abbreviation), the desired column names of the resulting separated columns (into = c(\"abbreviation\", \"description\")), and the characters that currently separate the pieces of information (sep = \" = \"). We have to put conservation abbreviation in back ticks in the code below because the column name contains a space. Without the back ticks, R would think that conservation and abbreviation were two separate things. This is another violation of tidy data! Variable names should have underscores, not spaces! conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) The output of this code shows that we now have two separate columns with the information in the original column separated out into abbreviation and description. Output of separate() 6.0.10 Merging Columns The opposite of separate() is unite(). So, if you have information in two or more different columns but wish it were in one single column, you’ll want to use unite(). Using the code forming the two separate columns above, we can then add on an extra line of unite() code to re-join these separate columns, returning what we started with. conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) %&gt;% unite(united_col, abbreviation, description, sep = &quot; = &quot;) Output of unite() 6.0.11 Cleaning up column names While maybe not quite as important as some of the other functions mentioned in this lesson, a function that will likely prove very helpful as you start analyzing lots of different datasets is clean_names() from the janitor package. This function takes the existing column names of your dataset, converts them all to lowercase letters and numbers, and separates all words using the underscore character. For example, there is a space in the column name for conservation. clean_names() will convert conservation abbreviation to conservation_abbreviation. These cleaned up column names are a lot easier to work with when you have large datasets. conservation %&gt;% clean_names() clean_names() output 6.0.12 Combining data across data frames There is often information stored in two separate data frames that you’ll want in a single data frame. There are many different ways to join separate data frames. They are discussed in more detail in this tutorial from Jenny Bryan. Here, we’ll demonstrate how the left_join() function works, as this is used frequently. Let’s try to combine the information from the two different datasets we’ve used in this lesson. We have msleep and conservation. msleep contains a column called conservation. This column includes lowercase abbreviations that overlap with the uppercase abbreviations in the abbreviation column in the conservation dataset. To handle the fact that in one dataset the abbreviations are lowercase and the other they are uppercase, we’ll use mutate() to take all the lowercase abbreviations to uppercase abbreviations using the function toupper(). We’ll then use left_join() which takes all of the rows in the first dataset mentioned (msleep, below) and incorporates information from the second dataset mentioned (conserve, below), when information in the second dataset is available. The by = argument states what columns to join by in the first (“conservation”) and second (“abbreviation”) datasets. This join adds the description column from the conserve dataset onto the original dataset (msleep). Note that if there is no information in the second dataset that matches with the information in the first dataset, left_join() will add NA. Specifically, for rows where conservation is “DOMESTICATED” below, the description column will have NA because “DOMESTICATED”” is not an abbreviation in the conserve dataset. ## take conservation dataset and separate information ## into two columns ## call that new object `conserve` conserve &lt;- conservation %&gt;% separate(`conservation abbreviation`, into = c(&quot;abbreviation&quot;, &quot;description&quot;), sep = &quot; = &quot;) ## now lets join the two datasets together msleep %&gt;% mutate(conservation = toupper(conservation)) %&gt;% left_join(conserve, by = c(&quot;conservation&quot; = &quot;abbreviation&quot;)) Data resulting from left_join It’s important to note that there are many other ways to join data, which are covered in more detail on this dplyr join cheatsheet from Jenny Bryan. For now, it’s important to know that joining datasets is done easily in R using tools in dplyr. As you join data frames in your own work, it’s a good idea to refer back to this cheatsheet for assistance. 6.0.13 Grouping Data Often, data scientists will want to summarize information in their dataset. You may want to know how many people are in a dataset. However, more often, you’ll want to know how many people there are within a group in your dataset. For example, you may want to know how many males and how many females there are. To do this, grouping your data is necessary. Rather than looking at the total number of individuals, to accomplish this, you first have to group the data by the gender of the individuals. Then, you count within those groups. Grouping by variables within dplyr is straightforward. 6.0.13.1 group_by() There is an incredibly helpful function within dplyr called group_by(). group_by() groups a dataset by one or more variables. On its own, it does not appear to change the dataset very much. The difference between the two outputs below is subtle: msleep msleep %&gt;% group_by(order) group_by() output In fact, the only aspect of the output that is different is that the number of different orders is now printed on your screen. However, in the next section, you’ll see that the output from any further functions you carry out at this point will differ between the two datasets. 6.0.14 Summarizing Data Throughout data cleaning and analysis it will be important to summarize information in your dataset. This may be for a formal report or for checking the results of a data tidying operation. 6.0.14.1 summarize() Continuing on from the previous examples, if you wanted to figure out how many samples are present in your dataset, you could use the summarize() function. msleep %&gt;% select(order) %&gt;% summarize(N=n()) This provides a summary of the data with the new column name we specified above (N) and the number of samples in the dataset. Note that we could also obtain the same information by directly obtaining the number of rows in the data frame with nrow(msleep). Summarize with n() However, if you wanted to count how many of each different order of mammal you had. You would first group_by(order) and then use summarize(). This will summarize within group. msleep %&gt;% group_by(order) %&gt;% select(order) %&gt;% summarize(N=n()) The output from this, like above, includes the column name we specified in summarize (N). However, it includes the number of samples in the group_by variable we specified (order). group_by() and summarize with n() There are other ways in which the data can be summarized using summarize(). In addition to using n() to count the number of samples within a group, you can also summarize using other helpful functions within R, such as mean(), median(), min(), and max(). For example, if we wanted to calculate the average (mean) total sleep each order of mammal got, we could use the following syntax: msleep %&gt;% group_by(order) %&gt;% select(order, sleep_total) %&gt;% summarize(N=n(), mean_sleep=mean(sleep_total)) summarize using mean() 6.0.14.2 tabyl() In addition to using summarize() from dplyr, the tabyl() function from the janitor package can be incredibly helpful for summarizing categorical variables quickly and discerning the output at a glance. Again returning to our msleep dataset, if we wanted to get a summary of how many samples are in each order category and what percent of the data fall into each category we could call tabyl on that variable. For example, if we use the following syntax, we easily get a quick snapshot of this variable. msleep %&gt;% tabyl(order) summarize using tabyl() from janitor Note, that tabyl assumes categorical variables. If you want to summarize numeric variables summary() works well. For example, this code will summarize the values in msleep$awake for you. summary(msleep$awake) summarize numeric variables 6.0.14.3 skim() When you would rather get a snapshot of the entire dataset, rather than just one variable, the skim() function from the skimr package can be very helpful. The output from skim() breaks the data up by variable type. For example, the msleep data set is broken up into character and numeric variable types. The data are then summarized in a meaningful way for each. This function provides a lot of information about the entire data set. So, when you want a summarize a dataset and quickly get a sense of your data, skim() is a great option! skim(msleep) summarize entire dataset using skim() from skimr 6.0.15 Conclusion We have gone through a number of ways to work with data in this lesson. Mastering the skills in this lesson will provide you with a number of critical data science skills. Thus, running the examples in this lesson and practicing on your own with other data sets will be essential to succeeding as a data scientist. 6.0.16 Additional Resources dplyr, part of the tidyverse janitor, from Sam Firke dplyr tutorials by Suzan Baert Part 1 Part 2 Part 3 Part 4 janitor tutorial by dplyr join cheatsheet by Jenny Bryan Note: a lot of the examples in this lesson were modified from the dplyr tutorials by Suzan Baert. To get an even deeper understanding of how to tidy data using dplyr, take a look at all of her dplyr tutorials. 6.0.17 Slides and Video Automated Video Slides "],["working-with-strings.html", "Chapter 7 Working with Strings", " Chapter 7 Working with Strings 7.0.1 Strings review Strings were introduced in an earlier lesson; however, to review briefly here: A string is a sequence of characters, letters, numbers or symbols. So within R, you could create a string using this syntax. Note that the string begins and ends with quotation marks: stringA &lt;- &quot;This sentence is a string.&quot; Multiple strings can be stored within vectors. So, if you have multiple vectors that you want to store in a single object, you could do so by using c() around the strings you want to store and commas to separate each individual string: objectA &lt;- c( &quot;This sentence is a string.&quot;, &quot;Short String&quot;, &quot;Third string&quot; ) 7.0.2 stringr stringr is a package within the tidyverse specifically designed to work well with strings. All functions within this package start with str_, as you’ll see below. There are many helpful functions within the stringr package. We’ll only review the basics here, but if you’re looking to accomplish something with a string and aren’t sure how to approach it, the stringr package is a good first place to look. To install and load the stringr package, you’ll use the following: # If not already installed, you&#39;ll need to install the package install.packages(&quot;stringr&quot;) # load package into R library(stringr) The best way to work through this lesson is to copy and paste every line of code into your RStudio window and see if the output makes sense to you. Working with strings and regular expressions is best learned by practice. 7.0.2.1 Available functions As we’ll only cover a few of the functions within stringr in this lesson, it’s important to remember that if you start typing “str_” within RStudio, a list of the many options will show up. str_ image 7.0.3 String basics When working with strings, some of the most frequent tasks you’ll need to complete are to: * determine the length of a string * combine strings together * subset strings 7.0.3.1 String length Returning to our object with three strings from earlier in the lesson, we can determine the length of each string in the vector. objectA &lt;- c( &quot;This sentence is a string.&quot;, &quot;Short String&quot;, &quot;Third string&quot; ) str_length(objectA) str_length output Here we see that the first string has a length of 26. If you were to go back and count the characters in the first string, you would see that this 26 includes each letter, space, and period in that string. The length of a string does not just could the letters in its length. The length includes every character. The second and third strings each have length 12. 7.0.3.2 Combining strings: str_c() If you were interested in combining strings, you’d want to use str_c. str_c( &quot;Good&quot;, &quot;Morning&quot;) str_c However, the output from this doesn’t look quite right. You may want a space between these two words when you combine the two strings. That can be controlled with the sep argument. str_c( &quot;Good&quot;, &quot;Morning&quot;, sep=&quot; &quot;) 7.0.3.3 Subsetting strings: str_sub() Often, it’s important to get part of a string out. To do this, you’ll want to subset the string using the str_sub() function. For example, if you wanted only the first three characters in the string below, you would specify that within str_sub(). object &lt;- c( &quot;Good&quot;, &quot;Morning&quot;) str_sub(object, 1, 3) str_sub output You can also use negative numbers to count from the end of the string. For example, below we see code that returns the last three positions in the string. object &lt;- c( &quot;Good&quot;, &quot;Morning&quot;) str_sub(object, -3, -1) str_sub output counting from end of string 7.0.3.4 String sorting: str_sort() Finally, if you wanted to sort a string alphabetically, str_sort() can help you accomplish that. names &lt;- c(&quot;Keisha&quot;, &quot;Mohammed&quot;, &quot;Jane&quot;) str_sort(names) str_sort() output sorts strings 7.0.4 Regular expressions Above we discuss the basics of working with strings within stringr. However, working with strings becomes infinitely easier with an understanding of regular expressions. Regular expressions (regexps) are used to describe patterns within strings. They can take a little while to get the hang of but become very helpful once you do. With regexps, instead of specifying that you want to extract the first three letters of a string (as we did above), you could more generally specify that you wanted to extract all strings that start with a specific letter or that contain a specific word somewhere in the string using regexps. We’ll explore the basics of regexps here. The use them in stringr, the general format is function(string , pattern = regexp), which you’ll see used in practice below. The set of functions from stringr we’ll cover are listed below We’ll cover a number of helpful stringr functions: str_view() - View the first occurrence in a string that matches the regex str_view_all() - View all occurrences in a string that match the regex str_count() - count the number of times a regex matches within a string str_detect() - determine if regex is found within string str_subset() - return subset of strings that match the regex str_extract() - return portion of each string that matches the regex str_replace() - replace portion of string that matches the regex with something else 7.0.4.1 Anchors If interested in finding a pattern at the beginning (^) or end ($) of a string, you can specify that using a regexp. For example, if you wanted to only look at names that started with the letter “M”, you would specify that using a regexp. The pattern you would include would be \"^M\" to identify all strings that start with the letter M. To specify those strings that end with a capital M, you would specify the pattern \"$M\". 7.0.4.2 Show matches: str_view() To get comfortable with using regexps with strings, str_view() can be very helpful. The output from str_view() highlights what portion of your string match the pattern specified in your regexp with a gray box. For example, to we’ll start using anchors and str_view() below: names &lt;- c(&quot;Keisha&quot;, &quot;Mohammed&quot;, &quot;Jane&quot;, &quot;Mathieu&quot;) ## identify strings that start with &quot;M&quot; str_view(names, &quot;^M&quot;) str_view() identifies names that start with M In this first example we see in the Viewer Panel that str_view has identified the names that start with the letter M. However, if you try to match strings that end with the letter “M”, no match is found. ## identify strings that end with &quot;M&quot; str_view(names, &quot;M$&quot;) str_view() does not identify any names that end with M To identify names by that end with the letter “a”, you would use the following. ## identify strings that end with &quot;a&quot; str_view(names, &quot;a$&quot;) str_view() identifies names that end with a Note, however, that regexps are case sensitive. To match patterns, you have to consider that “A” and “a” are different characters. ## identify strings that end with &quot;A&quot; str_view(names, &quot;A$&quot;) str_view() does not identify any names that end with A 7.0.4.3 Count matches: str_count() To count the number of matches within your strings, you would use str_count(). Below, using the names vector we’ve been using, we see that str_count() produces a 1 for those names that start with “M” and a 0 otherwise. ## identify strings that start with &quot;M&quot; ## return count of the number of times string matches pattern str_count(names, &quot;^M&quot;) str_count() strings that start with “M” However, if we instead wanted a count of the numbers of lowercase “m”s, we could still use str_count() to accomplish that. Notice below we’ve removed the specification to just look at the beginning of the string. Here, we’re looking for lowercase m’s anywhere in the string and counting them: ## identify strings that have a lowercase &quot;m&quot; ## return count of the number of times string matches pattern str_count(names, &quot;m&quot;) str_count() strings that have an m in them 7.0.4.4 Detect matches: str_detect() Instead of returning a count, at times you’re just interested in knowing which strings match the pattern you’re searching for. In these cases you’ll want to use str_detect(). This function simply returns a TRUE if the string matches the pattern specified and FALSE otherwise. ## identify strings that start with &quot;M&quot; ## return TRUE if they do; FALSE otherwise str_detect(names, &quot;^M&quot;) str_detect() returns TRUE for strings that match the specified pattern; FALSE otherwise 7.0.4.5 Subset matches: str_subset() To return the actual string that matches the specified pattern, rather than a TRUE/FALSE, you’ll look to str_subset(). This function pulls out those strings that match the specified pattern. For example, to obtain the subset of names whose values start with the capital letter “M”, you would use the following: ## identify strings that start with &quot;M&quot; ## return whole string str_subset(names, &quot;^M&quot;) str_subset() returns the strings that match the pattern specified 7.0.4.6 Extract matches: str_extract() To extract only the portions of the string that match the specified pattern, you would use str_extract(). This function returns the pattern specified for strings where it is found and NA otherwise. For example, by searching for names that start with M, below, we see that the second and fourth strings in our vector return the pattern specified (“M”) and that the first and third strings in the vector return NA, as they do not start with a capital “M”. ## return &quot;M&quot; from strings with &quot;M&quot; in it ## otherwise, return NA str_extract(names, &quot;^M&quot;) str_extract() returns the portions of the strings that match the pattern specified 7.0.4.7 Replace matches: str_replace() The final basic function from stringr that we’ll discuss is str_replace(). This function identifies a regex and replaces each occurrence with whatever replacement the user specifies. For example, below we search for strings that start with the capital letter “M” and replace each of them with a question mark. All strings that do not match the regex are returned unchanged. ## replace capital M with a question mark str_replace(names, &quot;^M&quot;, &quot;?&quot;) str_replace() replaces regex with specified characters 7.0.4.8 Common regular expressions Above we discuss two common patterns searched for using regular expressions: starts with (^) and ends with ($). However, there are a number of additional common ways to match patterns. They are listed here, and we’ll discuss each one in slightly more detail below. 7.0.4.8.1 Searching for characters To search for a set of characters, you place these characters within brackets. Below, this will identify anywhere in the strings where you have a lowercase vowel. Note, that we’re now using str_view_all() to identify all occurrences of these characters, rather than str_view(), which only identifies the first occurrence in each string. ## identify all lowercase vowels str_view_all(names, &quot;[aeiou]&quot;) brackets specify which characters to search for 7.0.4.8.2 Searching for anything other than a set of characters By adding a caret (^) before the vowels within the brackets, this regular expressions specifies that you are searching for any character that is not a lowercase vowel within your strings. ## identify anything that&#39;s NOT a lowercase vowel str_view_all(names, &quot;[^aeiou]&quot;) brackets with a caret first specify which characters NOT to search for 7.0.4.8.3 Search for digits To search for digits (numeric variable between 0 and 9) in a string you use “; however, backslashes are protected characters in R. This means that you have to escape this character first with an additional backslash (\\), to let R know that you want to search for the regular expression”. addresses &lt;- c(&quot;1234 Main Street&quot;, &quot;1600 Pennsylvania Ave&quot;, &quot;Brick Building&quot;) ## identify anything that&#39;s a digit str_view_all(addresses, &quot;\\\\d&quot;) earches for digits 7.0.4.8.4 Search for whitespace Identifying whitespace in R identifies any spaces, tabs or newlines. Note that again we have to escape the “” with a backslash for R to recognize the regular expression. ## identify any whitespace str_view_all(addresses, &quot;\\\\s&quot;) searches for whitespace 7.0.4.8.5 Identify any character (except newline) To identify any character except for a newline you’ll use \".\". Notice in our addresses example that there are no newlines, so this pattern will match with the entire string. ## identify any character str_view_all(addresses, &quot;.&quot;) . searches for any character 7.0.4.9 Repetition within regular expressions Searches for regular expressions allow you to specify how many times a pattern should be found within the string. To do so, you use the following: ? : 0 or 1 : 1 or more \\* : 0 or more {n} : exactly n times {n,} : n or more times {n,m} : between n and m times 7.0.4.9.1 Examples of repetition within regular expressions Using the definitions above, we can see that the following code will identify patterns within the addresses vector where n shows up one more more times in a string. ## identify any time n shows up one or more times str_view_all(addresses, &quot;n+&quot;) + specifies to match the pattern one or more times While the difference is slight in the output here, we’re identifying portions of the string where n shows up exactly once. So, instead of the ‘nn’ in Pennsylvania matching together, the code here splits these up, due to the fact that we’re specifying the pattern match ‘n’ exactly one time: ## identify any time n shows up str_view_all(addresses, &quot;n{1}&quot;) {#} looks to match the pattern exactly the number of times within the curly braces If you only wanted to match strings where n showed up twice in a row, you could specify that in this way: ## identify any time n shows up exactly two times in a row str_view_all(addresses, &quot;n{2}&quot;) {2} specifies that the pattern must be found exactly twice This could similarly be achieved by specifying to search for the pattern ‘nn’ one or more times (+): ## identify any time &#39;nn&#39; shows up one or more times str_view_all(addresses, &quot;nn+&quot;) nn+ searches for double n one or more times in a string You can also specify a range of the number of times to search for a pattern within your string. Below, we see that if we specify n be searched for at least two and at most 3 times, the pattern matches within our string. However, if we increase that to between three and four times, no pattern matching occurs, as there are never three or four n’s in a row in our strings. ## identify any time n shows up two or three times str_view_all(addresses, &quot;n{2,3}&quot;) ## identify any time n shows up three or four times str_view_all(addresses, &quot;n{3,4}&quot;) {n,m} looks to pattern match between n and m times 7.0.5 Conclusion This lesson set out to introduce you to how to work with strings within RStudio, using the stringr package and to introduce you to regular expressions. We’ve covered a number of functions and concepts in this lesson. Feel free to review the material and practice as much as you need before completing this quiz! 7.0.6 Additional Resources r4ds : Chapter 14 - Strings by Hadley Wickham stringr documentation, part of the tidyverse 7.0.7 Slides and Video Automated Video Slides "],["working-with-factors.html", "Chapter 8 Working with Factors", " Chapter 8 Working with Factors In R, categorical data are handled as factors. By definition, categorical data are limited in that they have a set number of possible values they can take. For example, there are 12 months in a calendar year. In a month variable, each observation is limited to taking one of these twelve values. Thus, with a limited number of possible values, month is a categorical variable. Categorical data, which will be referred to as factors for the rest of this lesson, are regularly found in data. Learning how to work with this type of variable effectively will be incredibly helpful. To make working with factors simpler, we’ll utilize the forcats package. Similar to the stringr package, all functions within forcats begin with fct_. As before, to see available functions you can type ?fct_ in your RStudio console. A drop-down menu will appear with all the possible forcats functions. Before working through this lesson, you’ll want to be sure that forcats has been installed and loaded in: install.packages(&#39;forcats&#39;) library(forcats) fct_ output from RStudio 8.0.1 Factor basics In R, factors are comprised of two components: the actual values of the data and the possible levels within the factor. Thus, to create a factor, you need to supply both these pieces of information. For example, if we were to create a character vector of the twelve months, we could certainly do that: ## all 12 months all_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) ## our data some_months &lt;- c(&quot;Mar&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Apr&quot;, &quot;Jul&quot;) However, if we were to sort this vector, R would sort this vector alphabetically. sort(some_months) sort sorts variable alphabetically While you and I know that this is not how months should be ordered, we haven’t yet told R that. To do so, we need to let R know that it’s a factor variable and what the levels of that factor variable should be. mon &lt;- factor(some_months, levels = all_months) mon sort(mon) defining the factor levels sorts this variable sensibly Here, we specify all the possible values that the factor could take in the levels = all_months argument. So, even though not all twelve months are included in the some_months object, we’ve stated that all of the months are possible values. Further, when you sort this variable, it now sorts in the sensical way! 8.0.2 Manually change the labels of factor levels : fct_relevel() What if you wanted your months to start with July first? That can be accomplished using fct_relevel(). To use this function, you simply need to state what you’d like to relevel (mon) followed by the levels you want to relevel. If you want these to be placed in the beginning, the after argument should be after = 0. You can play around with this setting to see how changing after affects the levels in your output. mon_relevel &lt;- fct_relevel(mon, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;, after = 0) mon_relevel sort(mon_relevel) fct_relevel enables you to change the order of your factor levels After re-leveling, when we sort this factor, we see that Jul is placed first, as specified by the level re-ordering. 8.0.3 Keep the order of the factor levels : fct_inorder() Now, if you’re not interested in the months being in calendar year order, you can always state that you want the levels to stay in the same order as the data you started with, you simply specify fct_inorder. mon_inorder &lt;- fct_inorder(some_months) mon_inorder sort(mon_inorder) fct_inorder assigns levels in the same order the level is seen in the data We see now with fct_inorder that even when we sort the output, it does not sort the factor alphabetically, nor does it put it in calendar order. In fact, it stays in the same order as the input, just as we specified. 8.0.4 Advanced Factoring For the remainder of this lesson, we’re going to return to using a dataset that’s in R by default. We’ll use the chickwts dataset for exploring the remaining advanced functions. This data set includes data from an experiment that was looking to compare the “effectiveness of various feed supplements on the growth rate of chickens.” chickwts dataset 8.0.5 Re-ordering factor levels by frequency : fct_infreq() To re-order factor levels by frequency of the value in the dataset, you’ll want to use fct_infreq(). Below, we see from the output from tabyl() that ‘soybean’ is the most frequent feed in the data set while ‘horsebean’ is the least frequent. Thus, when we order by frequency, we can expect these two feeds to be at opposite ends for our levels. ## take a look at frequency of each level ## using tabyl() from `janitor` package library(janitor) tabyl(chickwts$feed) ## order levels by frequency fct_infreq(chickwts$feed) %&gt;% head() fct_infreq orders levels based on frequency in dataset As expected, soybean, the most frequent level, appears as the first level and horsebean, the least frequent level, appears last. The rest of the levels are sorted by frequency. 8.0.6 Reversing order levels : fct_rev() If we wanted to sort the levels from least frequent to most frequent, we could just put fct_rev() around the code we just used to reverse the factor level order. ## reverse factor level order fct_rev(fct_infreq(chickwts$feed)) %&gt;% head() fct_rev() reverses the factor level order 8.0.7 Re-ordering factor levels by another variable : fct_reorder() At times you may want to reorder levels of a factor by another variable in your dataset. This is often helpful when generating plots (which we’ll get to in a future lesson!). To do this you specify the variable you want to reorder, followed by the numeric variable by which you’d like the factor to be re-leveled. Here, we see that we’re re-leveling feed by the weight of the chickens. While we haven’t discussed plotting yet, the best way to demonstrate how this works is by plotting the feed against the weights. We can see that the order of the factor is such that those chickens with the lowest median weight (horsebean) are to the left, while those with the highest median weight (casein) are to the right. ## order levels by a second numeric variable chickwts %&gt;% mutate(newfeed = fct_reorder(feed, weight)) %&gt;% ggplot(., aes(newfeed,weight)) + geom_point() fct_reorder allows you to re-level a factor based on a secondary numeric variable 8.0.8 Combining several levels into one: fct_recode() To demonstrate how to combine several factor levels into a single level, we’ll continue to use our ‘chickwts’ dataset. Now, I don’t know much about chicken feed, and there’s a good chance you know a lot more. However, let’s assume (even if it doesn’t make good sense with regards to chicken feed) you wanted to combine all the feeds with the name “bean” in it to a single category and you wanted to combine “linseed” and “sunflower”” into the category “seed”. This can be simply accomplished with fct_recode. In fact, below, you see we can rename all the levels to a simpler term (the values on the left side of the equals sign) by re-naming the original level names (the right side of the equals sign). This code will create a new column, called feed_recode (accomplished with mutate()). This new column will combine “horsebean” and “soybean feeds”, grouping them both into the larger level “bean”. It will similarly group “sunflower” and “linseed” into the larger level “seed.” All other feed types will also be renamed. When we look at the summary of this new column by using tabyl(), we see that all of the feeds have been recoded, just as we specified! We now have four different feed types, rather than the original six. ## we can use mutate to create a new column ## and fct_recode() to: ## 1. group horsebean and soybean into a single level ## 2. rename all the other levels. chickwts %&gt;% mutate(feed_recode = fct_recode(feed, &quot;seed&quot; = &quot;linseed&quot;, &quot;bean&quot; = &quot;horsebean&quot;, &quot;bean&quot; = &quot;soybean&quot;, &quot;meal&quot; = &quot;meatmeal&quot;, &quot;seed&quot; = &quot;sunflower&quot;, &quot;casein&quot; = &quot;casein&quot; )) %&gt;% tabyl(feed_recode) fct_recode() can be used to group multiple levels into a single level and/or to rename levels 8.0.9 Converting numeric levels to factors: ifelse() + factor() Finally, when working with factors, there are times when you want to convert a numeric variable into a factor. For example, if you were talking about a dataset with BMI for a number of individuals, you may want to categorize people based on whether or not they are underweight (BMI &lt; 18.5), of a healthy weight (BMI between 18.5 and 29.9), or obese (BMI &gt;= 30). When you want to take a numeric variable and turn it into a categorical factor variable, you can accomplish this easily by using ifelse() statements. if{} statements and else{} statements were covered in an earlier lesson. Here we combine those two ideas. Within a single statement we provide R with a condition: weight &lt;= 200. With this, we are stating that the condition is if a chicken’s weight is less than or equal to 200 grams. Then, if that condition is true, meaning if a chicken’s weight is less than or equal to 200 grams, let’s assign that chicken to the category low. Otherwise, and this is the else{} part of the ifelse() function, assign that chicken to the category high. Finally, we have to let R know that weight_recode is a factor variable, so we call factor() on this new column. This way we take a numeric variable (weight), and turn it into a factor variable (weight_recode). ## convert numeric variable to factor chickwts %&gt;% mutate(weight_recode = ifelse(weight &lt;= 200, &quot;low&quot;, &quot;high&quot;), weight_recode = factor(weight_recode)) %&gt;% tabyl(weight_recode) converting a numeric type variable to a factor 8.0.10 Conclusions This lesson has covered how to work with factors (categorical) variables in R using the forcats package in R. In line with the previous few lessons, the best way to learn how to work with factors, is to actually work with factors. Play around with the examples here and continue to practice using this type of variable! You’ll come across factors regularly as you analyze data! 8.0.11 Additional Resources r4ds : Chapter 15 - Factors by Hadley Wickham forcats, part of the tidyverse forcats blog post by Hadley Wickham Wrangling Categorical Data in R by Amelia McNamara &amp; Nicholas J Horton Note: Wrangling Categorical Data in R is full of really great examples and goes into further depth than what is covered here. This is a great resource to get more practice working with categorical data (factors) in R. 8.0.12 Slides and Video Automated Video Slides "],["working-with-dates.html", "Chapter 9 Working with Dates", " Chapter 9 Working with Dates In lessons an earlier course, you were introduced to different types of objects in R, such as characters, numeric, and logicals. Then, in earlier lessons in this course, we covered how to work with strings and factors in detail. The remaining type of variable we haven’t yet covered is how to work with dates and time in R. As with strings and factors, there is a tidyverse package to help you work with dates more easily. The lubridate package will make working with dates and times easier. Before working through this lesson, you’ll want to be sure that lubridate has been installed and loaded in: install.packages(&#39;lubridate&#39;) library(lubridate) 9.0.1 Dates and time basics When working with dates and times in R, you can consider either dates, times, or date-times. Date-times refer to dates plus times, specifying an exact moment in time. It’s always best to work with the simplest possible object for your needs. So, if you don’t need to refer to date-times specifically, it’s best to work with dates. 9.0.2 Creating dates and date-time objects To get objects into dates and date-times that can be more easily worked with in R, you’ll want to get comfortable with a number of functions from the lubridate package. Below we’ll discuss how to create date and date-time objects from (1) strings and (2) individual parts. 9.0.2.1 From strings Date information is often provided as a string. The functions within the lubridate package can effectively handle this information. To use them to generate date objects, you can call a function using y, m, and d in the order in which the year (y), month (m), and date (d) appear in your data. The code below produces identical output for the date September 29th, 1988, despite the three distinct input formats. This uniform output makes working with dates much easer in R. ymd(&quot;1988-09-29&quot;) mdy(&quot;September 29th, 1988&quot;) dmy(&quot;29-Sep-1988&quot;) creating date and date-time objects However, this has only covered working with date objects. To work with date-time objects, you have to further include hour (h), minute(m), and second (s) into the function. For example, in the code below, you can see that the output contains time information in addition to the date information generated in the functions above: ymd_hms(&quot;1988-09-29 20:11:59&quot;) 9.0.2.2 From individual parts If you have a data set where month, date, year, and/or time information are included in separate columns, the functions within lubridate can take this separate information and create a date or date-time object. To work through examples using the functions make_date() and make_timedate(), we’ll use a dataset called nycflights13. As this dataset is not included with the R by default, you’ll have to install and load it in directly: install.packages(&#39;nycflights13&#39;) library(nycflights13) Loading this package makes a data frame called flights, which includes “on-time data for all flights that departed NYC in 2013,” available. We will work with this dataset to demonstrate how to create a date and date-time object from a dataset where the information is spread across multiple columns. First, to create a new column, as we’ve done throughout the lessons in this course, we will use mutate(). To create a date object, we’ll use the function make_date(). We just then need to supply the names of the columns containing the year, month, and day information to this function. ## make_date() creates a date object ## from information in separate columns flights %&gt;% select(year, month, day) %&gt;% mutate(departure = make_date(year, month, day)) mutate and make_date() create a new column – departure – with a date object A similar procedure is used to create a date-time object; however, this requires the function make_datetime() and requires columns with information about time be specified. Below, hour and minute are included to the function’s input. ## make_datetime() creates a date-time object ## from information in separate columns flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(departure = make_datetime(year, month, day, hour, minute)) mutate and make_datetime() create a new column – departure – with a date-time object 9.0.3 Working with dates The reason we’ve dedicated an entire lesson to working with dates and have shown you how to create date and date-time objects in this lesson is because you often want to plot data over time or calculate how long something has taken. Being able to accomplish these tasks is an important job for a data scientist. So, now that you know how to create date and date-time objects, we’ll work through a few examples of how to work with these objects 9.0.3.1 Getting components of dates Often you’re most interested in grouping your data by year, or just looking at monthly or weekly trends. To accomplish this, you have to be able to extract just a component of your date object. You can do this with the functions: year(), month(), mday(),wday(), hour(), minute() and second(). Each will extract the specified piece of information from the date or date-time object. mydate &lt;- ymd(&quot;1988-09-29&quot;) ## extract year information year(mydate) ## extract day of the month mday(mydate) ## extract weekday information wday(mydate) ## label with actual day of the week wday(mydate, label = TRUE) lubridate has specific functions to extract components from date and date-time objects 9.0.4 Time spans In addition to being able to look at trends by month or year, which requires being able to extract that component from a date or date-time object, it’s also important to be able to operate over dates. If I give you a date of birth and ask you how old that person is today, you’ll want to be able to calculate that. This is possible when working with date objects. By subtracting this birthdate from today’s date, you’ll learn now many days old this person is. By specifying this object using as.duration(), you’ll be able to extract how old this person is in years. ## how old is someone born on Sept 29, 1988 mydate &lt;- ymd(&quot;1988-09-29&quot;) ## subtract birthday from todays date age &lt;- today() - mydate age ## a duration object can get this information in years as.duration(age) dates and date-times can be operated upon Using addition, subtraction, multiplication, and division is possible with date objects, and accurately takes into account things like leap years and different number of days each month. This capability and the additional functions that exist within lubridate can be enormously helpful when working with dates and date-time objects. 9.0.5 What’s not covered in this lesson This lesson has not covered how to work with times, much detail about how to operate on date or date-time objects, nor how to deal with timezones in date-time objects. To learn more about these subjects, feel free to explore the additional resources below, or hold off for a later course in a different course set! 9.0.6 Additional Resources r4ds : Chapter 16 - Dates and times by Hadley Wickham lubridate, part of the tidyverse hms package, for working with time objects. This package is also part of the tidyverse 9.0.7 Slides and Video Automated Video Slides "],["data-tidying.html", "Chapter 10 Data Tidying", " Chapter 10 Data Tidying Throughout this course set, we’ll have a number of projects for you to complete. These will be included as Exercises. This means they will not be required to pass the course and receive your certificate; however, completing them will really help to improve your understanding of the material covered and to ensure that you’ve begun to master the skills needed to be a data scientist. In each project, we’ll aim to get you started and to ask questions that will help guide you through the project. But, we’ll intentionally leave pieces out where you have to figure out what needs to be done. This first project will require you to: use version control demonstrate the skills you’ve learned programming in R wrangle data You can access the first project by going to the exercise accompanying this lesson. "],["data-tidying-project.html", "Chapter 11 Data Tidying Project", " Chapter 11 Data Tidying Project * If you would like to know the answers to the questions in this exercise, then you can take this course in Leanpub. Often times, data scientists are handed data and asked to make sense of them. The data scientist may be asked to figure out why people are getting sick in a city or how their employer can save money. No matter the situation, these data aren’t always in the most usable format. Data wrangling is required before any pretty visualizations can be made or interesting questions can be answered. As such, this exercise has been generated to practice your, GitHub, terminal navigation, RStudio, and data wrangling skills. 11.0.0.1 GitHub Setup To get started, you’ll want to go to GitHub and start a new repository: Call this repository data_tidying_project. Add a short description Check the box to “Initialize this repository with a README. Click Create Repository Once the repository has been created, Click on Clone or download and copy the “Clone with HTTPS” link provided. You’ll use this to clone your repo in RStudio Cloud. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. 11.0.0.2 RStudio Cloud Setup Go to the Cloud-based Data Science Space on RStudio Cloud Click on the “Projects” tab at the top of the workspace Make a copy of the project: data_tidying_project In this project you should see a data_tidying_project.Rmd file. You’ll use this to get started working on your project! Note: If you try to Knit this document at this time, you will get an error because there is code in this document that has to be edited (by you!) before it will be able to successfully knit! To start using version control, you’ll want to clone the GitHub repository you just created into this workspace. To do so, go to the Terminal and clone your project into this workspace. A new directory with the name of your GitHub repository should now be viewable in the Files tab of RStudio Cloud. You are now set up to track your project with git. Why did you clone your GitHub repository into RStudio Cloud? 11.0.0.3 Data Science Project Setup As discussed previously, you’ll want all your data science projects you be organized from the very beginning. Let’s do that now! First, use cd to get yourself into the directory of your GitHub Project. Once in the correct directory, use mkdir in the terminal to create folders with the following structure: \\- data/ \\- raw_data/ \\- tidy_data/ \\- code/ \\- raw_code/ \\- final_code/ \\- figures/ \\- exploratory_figures/ \\- explanatory_figures/ \\- products/ Now that your directories are set up you’ll want to use the Terminal (or ‘More’ drop-down menu in the Files tab) to move (mv) the data_tidying_project.Rmd file into code/raw_code. This ensures that your code file is in the correct directory. In what directory did you save data_tidying_project.Rmd? Once the .Rmd document is in the correct folder, you’ll want to change the author of this document to your name at the top of the .Rmd document (in the YAML). Save this change before moving to the next step. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Organizing Data Science Projects. Refer to the materials in this course if you’re stuck on this part of the project. 11.0.0.4 Pushing to GitHub You’ll want to save changes to your project regularly by pushing them to GitHub. Now that you’ve got your file structure set up and have added an R Markdown document to your code/raw_code directory, it’s a good time to stage, commit, and push these changes to GitHub. Do so now, and then take a long on GitHub to see the changes on their website! After this initial push, how many of the directories you created are visible on GitHub? Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. 11.0.0.5 The Data To get you started, let’s get you acquainted with the data. The data you’ll be working with are hosted at data.world and contain information about Sales in the US. You’ll be working with two datasets. Data Set 1: Sales from the Retail Trade and Food Services Report from the US Census. This dataset only covers Department Stores, though the report covers a wide range of retail types. [1992-2016] Data Set 2 US Retail Sales by Store Type with Growth Rate [2009-2014] 11.0.0.6 Getting the Data into R To get the data read into R, we’re going to use two packages: httr and readxl. These packages will be covered in the next course. As they haven’t been taught yet, we’ll have you use them here, but we’ll provide all the code you need in order to use them, rather than having you figure this part out on your own. You can see the code to get these data read into RStudio in the second code chunk (data) provided within data_tidying_project.Rmd. To read the data in, first run the code in the first code chunk ( setup) to get the necessary packages loaded into R. Then, run the code in the second code chunk (data) to load the data into R. In your Environment tab, you will see that there are two new objects, df1 and df2 that have been created. Once the data are read in, to take a look at these data you can use the glimpse() or View() functions in RStudio Cloud (i.e. View(df1) or glimpse(df1) 11.0.0.7 Saving the Raw Data While the data are available on the Internet, what if in the future they take them down? Then, you’ll have lost the data for your project or at the very least you won’t have access to it. To avoid this being a problem in the future, let’s save a copy of the data on RStudio Cloud now. You’ll want to save these data in the data/raw_data folder you created. We’ll save these as .rds objects. This means that each has to be saved separately using the saveRDS() function. Add the code to do this to the save-data code chunk in your data_tidying_project.Rmd document. Save df1 as ‘df_department.rds’ and df2 as df_retail.rds. What line of code did you use to save df1? 11.0.0.8 Wrangle the Data We’ll now work with the two datasets you read in, so that they’re in a usable (long) data format and can be merged into a single data frame. The wrangling portion of this project will use the skills you learned in Introduction to R and this course Data Tidying. Feel free to refer back to these materials for reference as you complete the project. 11.0.0.8.1 The retail data: df2 Now that you have the data in RStudio Cloud and have saved the raw data to your data/raw_data file, we’re ready to start wrangling data. To get you acquainted with the dataset, we’ll first have you answer a few questions about the dataset: df2 How many rows are there in df2 How many columns are there in df2 In 2014, what were the sales for used car dealers? Now, the goal of wrangling these data will be to take this data frame, which is currently in the wide data format and wrangle it into a long data format. The resulting data frame from your wrangling will have three columns: business, year, and n. In the data_tidying_project.Rmd document, you’ll see the code required to accomplish this in the wrangle-df2 code chunk. We recommend running each line at a time. For example, start by running: ## an example working with df2 ## let&#39;s wrangle! df_retail &lt;- df2 %&gt;% ## remove the r from the column names of df2 magrittr::set_colnames(gsub(&quot;r&quot;,&quot;&quot;,df2[1,])) Take a look at the output variable df_retail (using glimpse(), View(), or skim()) to see what the magrittr::set_colnames() code accomplishes. Once you understand this line, then move onto the next line and run: ## an example working with df2 ## let&#39;s wrangle! df_retail &lt;- df2 %&gt;% ## remove the r from the column names of df2 magrittr::set_colnames(gsub(&quot;r&quot;,&quot;&quot;,df2[1,])) %&gt;% ## add a new column called &quot;business&quot; mutate(business = gsub(&quot;[?]|[.]&quot;,&quot;&quot;,`Kind of business`)) Again, take a look at the output variable df_retail to understand this additional line of code. This will help you understand what each line of code does! And, you’ll understand why each step was taken to get the data into it’s final format. Continue this process until you understand the entire chunk of code in wrangle-df2. If you were to change the line mutate(business = gsub(\"[?]|[.]\",\"\",Kind of business)) to mutate(biz = gsub(\"[?]|[.]\",\"\",Kind of business)), what would change? What information is in the column n? What does filter(business == \"Retail sales, total \"| business==\"Department stores \") accomplish? 11.0.0.8.2 The department store data: df1 Having had all the code to wrangle df2 and being able to understand what each line did, now it’s your turn to wrangle a data frame! You’ll take We’ll first ask a few questions to get you acquainted with df1. How many rows are there in df1 How many columns are there in df1 In Jun-1992, what were retail sales (in millions)? Now that you have an idea of what data are in the data frame df, it’s time to wrangle! The goal of wrangling this data set is to mirror (or mimic) what we did in the first data set. This means, that by the end of data wrangling, you should have three columns with the same names as df_retail: business, year, and n. To accomplish this goal, you’ll have to figure out what code should replace function_name for each line in the code chunk wrangle-df1. A different function_name will be called in each line of missing code. The arguments necessary to be included within the parentheses will also have to be added. For example, for the first part of the code, you currently see in the df_department &lt;- df1 %&gt;% ## split Period column into one column called &quot;month&quot; and one called &quot;year&quot; function_name() The comment ## split Period column into one column called \"month\" and one called \"year\" tells you what you’ll want to accomplish in the next line of code. So, you may try adding the following: df_department &lt;- df1 %&gt;% ## split Period column into one column called &quot;month&quot; and one called &quot;year&quot; separate(Period, into = c(&#39;month&#39;,&#39;year&#39;), extra = &#39;drop&#39;, remove = FALSE) By changing function_name() to the appropriate function separate() and including the necessary code within the parentheses, we are able to accomplish exactly what the comment says we wanted to do! Before you get started on the rest of this, a quick note on data wrangling : As you master these skills, you will write code that errors and things will take you multiple tries before you get it right. If you’re frustrated, that’s ok! It’s part of the process. Use the material in this course or places like StackOverflow to help you if you find answers if you get stuck!. OK, now it’s your job to work line by line to change function_name to the appropriate function in order to accomplish what the comments state you want to accomplish. Work through this code chunk line by line until you have a long data set called df_department with three columns: business, year, and n. What function did you use to group the data by the year column? What function did you use to add the column name value? What function did you use to reorder the column names? 11.0.0.8.3 Pushing to GitHub You’ve written a lot of code at this point, so it’d be great to add this to GitHub at this point. Use git add, git commit, and git push to add the file changes to your GitHub repository. 11.0.0.8.4 Merging the data Once you have wrangled both df1 and df2, you should be able to join them by binding the rows of df_retail and df_department. Change the code in the merge-data code chunk to generate an object called df_total. This should only take one function to accomplish. 11.0.0.8.5 Plotting Your Data While data visualization is coming up in a later course, we’ve included a few lines of code to plot the data from the three data frames you created (df_retail, df_department, df_total). For this you’ll use the R package ggplot2. Try running these lines of code in the plot chunk and take a look at the plots generated in the “Plots” tab of RStudio Cloud. However, don’t stress if you’re not exactly sure what’s going on. We’ll cover all of this in an upcoming course! 11.0.0.9 Add Markdown Text to .Rmd Before finalizing your project you’ll want to add some text outside of your code chunks to explain what you’re doing in each code chunk. These explanations are incredibly helpful for someone who doesn’t code. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Introduction to R. Refer to the R Markdown lesson in this course if you’re stuck on this part (or the next part) of the project. 11.0.0.10 Knit your R Markdown Document Last but not least, you’ll want to Knit your .Rmd document into an HTML document. If you get an error, take a look at what the error says and edit your .Rmd document. Then, try to Knit again! Troubleshooting these error messages will teach you a lot about coding in R. 11.0.0.11 Final push to GitHub Now that you’ve finalized your project, you’ll do one final push to GitHub. add, commit, and push your work to GitHub. Navigate to your GitHub repository, and answer the final question below! Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. Congrats on finishing your first Data Science Project "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) FirstName LastName Lecturer(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved Delivered the course in some way - video or audio Content Author(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved If any other authors besides lead instructor Content Contributor(s) (include section name/link in parentheses) - make new line if more than one section involved Wrote less than a chapter Content Editor(s)/Reviewer(s) Checked your content Content Director(s) Helped guide the content direction Content Consultants (include chapter name/link in parentheses or word “General”) - make new line if more than one chapter involved Gave high level advice on content Acknowledgments Gave small assistance to content but not to the level of consulting Production Content Publisher(s) Helped with publishing platform Content Publishing Reviewer(s) Reviewed overall content and aesthetics on publishing platform Technical Course Publishing Engineer(s) Helped with the code for the technical aspects related to the specific course generation Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator(s) Created graphics for the course Figure Artist(s) Created figures/plots for course Videographer(s) Filmed videos Videography Editor(s) Edited film Audiographer(s) Recorded audio Audiography Editor(s) Edited audio recordings Funding Funder(s) Institution/individual who funded course including grant number Funding Staff Staff members who help with funding   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2023-03-20 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## bookdown 0.24 2022-02-15 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.0.2) ## hms 0.5.3 2020-01-08 [1] RSPM (R 4.0.0) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## knitr 1.33 2022-02-15 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.0.2) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## ottrpal 0.1.2 2022-02-15 [1] Github (jhudsl/ottrpal@1018848) ## pillar 1.4.6 2020-07-10 [1] RSPM (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.0.3) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## readr 1.4.0 2020-10-05 [1] RSPM (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2022-02-15 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2022-02-15 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2022-02-15 [1] Github (R-lib/testthat@e99155a) ## tibble 3.0.3 2020-07-10 [1] RSPM (R 4.0.2) ## usethis 2.1.5.9000 2022-02-15 [1] Github (r-lib/usethis@57b109a) ## vctrs 0.3.4 2020-08-29 [1] RSPM (R 4.0.2) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2022-02-15 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["references.html", "Chapter 12 References", " Chapter 12 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
